{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e996dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from models import BaseVAE\n",
    "from torch.nn import functional as F\n",
    "from models.simple_vae import SimpleVAE  #The model that we simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "396c544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"n_samples\": 4000,\n",
    "    \"latent_dim\": 2,\n",
    "    \"hidden_dims\": [64, 64, 64, 64], \n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"beta_values\": [0.0, 0.5, 1.0, 5.0],\n",
    "    \"plot_dir\": \"plots\",\n",
    "    \"seed\": 42,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def generate_swiss_roll(n_samples):\n",
    "    \"\"\"\n",
    "    Generate Swiss Roll(3D)\n",
    "    \"\"\"\n",
    "    s = np.random.uniform(1.5 * np.pi, 4.5 * np.pi, n_samples).astype(np.float32)\n",
    "    t = np.random.uniform(0, 15, n_samples).astype(np.float32)\n",
    "    \n",
    "    x = s * np.cos(s)\n",
    "    y = t\n",
    "    z = s * np.sin(s)\n",
    "    coords = np.stack([x, y, z], axis=1)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    mins, maxs = coords.min(axis=0), coords.max(axis=0)\n",
    "    data = 2.0 * (coords - mins) / (maxs - mins + 1e-8) - 1.0\n",
    "    \n",
    "    s_norm = (s - s.min()) / (s.max() - s.min() + 1e-8)\n",
    "    t_norm = (t - t.min()) / (t.max() - t.min() + 1e-8)\n",
    "    \n",
    "    return data.astype(np.float32), s_norm, t_norm, s, t\n",
    "\n",
    "\n",
    "def generate_three_circles(n_samples):\n",
    "    \"\"\"\n",
    "    Generate three nested circles (2D).\n",
    "    \"\"\"\n",
    "    n_per_circle = n_samples // 3\n",
    "    \n",
    "    radii = [0.3, 0.6, 0.9]\n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, r in enumerate(radii):\n",
    "        theta = np.random.uniform(0, 2 * np.pi, n_per_circle)\n",
    "        noise = np.random.normal(0, 0.02, n_per_circle)\n",
    "        x = (r + noise) * np.cos(theta)\n",
    "        y = (r + noise) * np.sin(theta)\n",
    "        data_list.append(np.stack([x, y], axis=1))\n",
    "        labels.extend([i] * n_per_circle)\n",
    "    \n",
    "    data = np.vstack(data_list).astype(np.float32)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Normalize theta for coloring (angle around circle)\n",
    "    theta_all = np.arctan2(data[:, 1], data[:, 0])\n",
    "    theta_norm = (theta_all + np.pi) / (2 * np.pi)\n",
    "    \n",
    "    return data, theta_norm, labels\n",
    "\n",
    "\n",
    "def generate_smile_face(n_samples=500):\n",
    "    \"\"\"\n",
    "    Generate a simple smile face (2D).\n",
    "    \"\"\"\n",
    "    n_face = n_samples // 2\n",
    "    n_eye = n_samples // 8\n",
    "    n_smile = n_samples - n_face - 2 * n_eye\n",
    "    \n",
    "    data_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Face outline (circle)\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_face)\n",
    "    noise = np.random.normal(0, 0.02, n_face)\n",
    "    x = (0.9 + noise) * np.cos(theta)\n",
    "    y = (0.9 + noise) * np.sin(theta)\n",
    "    data_list.append(np.stack([x, y], axis=1))\n",
    "    labels.extend([0] * n_face)\n",
    "    \n",
    "    # Left eye (small filled circle)\n",
    "    r = np.random.uniform(0, 0.1, n_eye)\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_eye)\n",
    "    x = -0.35 + r * np.cos(theta)\n",
    "    y = 0.3 + r * np.sin(theta)\n",
    "    data_list.append(np.stack([x, y], axis=1))\n",
    "    labels.extend([1] * n_eye)\n",
    "    \n",
    "    # Right eye (small filled circle)\n",
    "    r = np.random.uniform(0, 0.1, n_eye)\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_eye)\n",
    "    x = 0.35 + r * np.cos(theta)\n",
    "    y = 0.3 + r * np.sin(theta)\n",
    "    data_list.append(np.stack([x, y], axis=1))\n",
    "    labels.extend([2] * n_eye)\n",
    "    \n",
    "    # Smile (arc)\n",
    "    theta = np.random.uniform(-0.8 * np.pi, -0.2 * np.pi, n_smile)\n",
    "    noise = np.random.normal(0, 0.02, n_smile)\n",
    "    x = (0.5 + noise) * np.cos(theta)\n",
    "    y = (0.5 + noise) * np.sin(theta) + 0.1\n",
    "    data_list.append(np.stack([x, y], axis=1))\n",
    "    labels.extend([3] * n_smile)\n",
    "    \n",
    "    data = np.vstack(data_list).astype(np.float32)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Color by angle from center\n",
    "    theta_all = np.arctan2(data[:, 1], data[:, 0])\n",
    "    theta_norm = (theta_all + np.pi) / (2 * np.pi)\n",
    "    \n",
    "    return data, theta_norm, labels\n",
    "\n",
    "\n",
    "def generate_single_circle(n_samples=500):\n",
    "    \"\"\"\n",
    "    Generate a single circle (2D).\n",
    "    \"\"\"\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_samples).astype(np.float32)\n",
    "    noise = np.random.normal(0, 0.02, n_samples).astype(np.float32)\n",
    "    \n",
    "    x = (1.0 + noise) * np.cos(theta)\n",
    "    y = (1.0 + noise) * np.sin(theta)\n",
    "    data = np.stack([x, y], axis=1).astype(np.float32)\n",
    "    \n",
    "    theta_norm = (theta - theta.min()) / (theta.max() - theta.min() + 1e-8)\n",
    "    \n",
    "    return data, theta_norm, theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f364c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train_vae(data, beta, config):\n",
    "    device = get_device()\n",
    "\n",
    "    # Turn numpy data to TensorDataset\n",
    "    dataset = TensorDataset(torch.from_numpy(data).float())\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=config[\"batch_size\"],\n",
    "                        shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = SimpleVAE(\n",
    "        input_dim=data.shape[1],\n",
    "        latent_dim=config[\"latent_dim\"],\n",
    "        hidden_dims=config.get(\"hidden_dims\", None), \n",
    "        beta=beta,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=config[\"learning_rate\"])\n",
    "    history = {\"loss\": [], \"recon\": [], \"kld\": [], \"beta\": []}\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        current_beta = beta\n",
    "        model.beta = current_beta  # keep internal beta\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss, epoch_recon, epoch_kld = 0.0, 0.0, 0.0\n",
    "\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward: [recons, input, mu, log_var]\n",
    "            recons, x_in, mu, log_var = model(batch)\n",
    "\n",
    "            # Minibatch weight for KL term (M_N ≈ B / N)\n",
    "            B = batch.size(0)\n",
    "            kld_weight = B / dataset_size\n",
    "\n",
    "            loss_dict = model.loss_function(\n",
    "                recons, x_in, mu, log_var, M_N=kld_weight\n",
    "            )\n",
    "            loss = loss_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate epoch stats\n",
    "            epoch_loss += loss.item() * B\n",
    "            epoch_recon += loss_dict[\"Reconstruction_Loss\"].item() * B\n",
    "            epoch_kld += (-loss_dict[\"KLD\"].item()) * B  \n",
    "\n",
    "        n_total = dataset_size\n",
    "        history[\"loss\"].append(epoch_loss / n_total)\n",
    "        history[\"recon\"].append(epoch_recon / n_total)\n",
    "        history[\"kld\"].append(epoch_kld / n_total)\n",
    "        history[\"beta\"].append(current_beta)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def get_latent_embeddings(model, data):\n",
    "    \"\"\"Get latent means for all data points.\"\"\"\n",
    "    device = get_device()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(torch.from_numpy(data).to(device))\n",
    "        return mu.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff925b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_plot_dir(plot_dir):\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def _scatter_2d(ax, data, labels=None, colors=None, cmap='viridis',\n",
    "                s=10, alpha=0.7, add_legend=True):\n",
    "    if labels is not None and len(np.unique(labels)) <= 10:\n",
    "        for lab in np.unique(labels):\n",
    "            mask = labels == lab\n",
    "            ax.scatter(data[mask, 0], data[mask, 1], s=s, alpha=alpha, label=f'Class {lab}')\n",
    "        if add_legend:\n",
    "            ax.legend()\n",
    "    else:\n",
    "        ax.scatter(data[:, 0], data[:, 1], c=colors, cmap=cmap, s=s, alpha=alpha)\n",
    "\n",
    "\n",
    "def _train_across_betas(data, config):\n",
    "    models, histories = [], []\n",
    "    for beta in config[\"beta_values\"]:\n",
    "        model, history = train_vae(data, beta, config)\n",
    "        models.append(model)\n",
    "        histories.append(history)\n",
    "    return models, histories\n",
    "\n",
    "\n",
    "# 2D Plots #\n",
    "def plot_2d_data(data, colors, labels, title, plot_dir, filename):\n",
    "    \"\"\"Plot 2D dataset.\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    _scatter_2d(ax, data, labels=labels, colors=colors)\n",
    "    if labels is None or len(np.unique(labels)) > 10:\n",
    "        sc = ax.collections[0]\n",
    "        plt.colorbar(sc, ax=ax)\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, filename), dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_2d_latent_comparison(models, beta_values, data, colors, labels, plot_dir, prefix):\n",
    "    \"\"\"Compare latent spaces for 2D input data.\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    n_betas = len(beta_values)\n",
    "    fig, axes = plt.subplots(1, n_betas + 1, figsize=(4 * (n_betas + 1), 4))\n",
    "\n",
    "    # Original data\n",
    "    _scatter_2d(axes[0], data, labels=labels, colors=colors, add_legend=False)\n",
    "    axes[0].set_title('Original Data')\n",
    "    axes[0].set_aspect('equal')\n",
    "\n",
    "    # Latent embeddings for each beta\n",
    "    for i, (model, beta) in enumerate(zip(models, beta_values), start=1):\n",
    "        mu = get_latent_embeddings(model, data)\n",
    "        _scatter_2d(axes[i], mu, labels=labels, colors=colors, add_legend=False)\n",
    "        axes[i].set_title(f'β={beta}')\n",
    "        axes[i].set_xlabel('$z_1$')\n",
    "        axes[i].set_ylabel('$z_2$')\n",
    "        axes[i].set_aspect('equal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, f\"{prefix}_latent_comparison.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_2d_reconstruction(models, beta_values, data, colors, labels, plot_dir, prefix):\n",
    "    \"\"\"Show reconstruction quality for 2D data.\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    device = get_device()\n",
    "    n_betas = len(beta_values)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_betas + 1, figsize=(4 * (n_betas + 1), 4))\n",
    "\n",
    "    # Original data\n",
    "    _scatter_2d(axes[0], data, labels=labels, colors=colors, add_legend=False)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].set_aspect('equal')\n",
    "\n",
    "    for i, (model, beta) in enumerate(zip(models, beta_values), start=1):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.from_numpy(data).float().to(device)\n",
    "            x_recon, _, _, _ = model(x)   # unpack 4 outputs\n",
    "            x_recon = x_recon.cpu().numpy()\n",
    "\n",
    "        # Reconstruction\n",
    "        _scatter_2d(axes[i], x_recon, labels=labels, colors=colors, add_legend=False)\n",
    "        axes[i].set_title(f'Recon β={beta}')\n",
    "        axes[i].set_aspect('equal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, f\"{prefix}_reconstruction.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# 3D Plot\n",
    "def plot_swiss_roll_3d(data, s_norm, t_norm, plot_dir):\n",
    "    \"\"\"Plot the 3D Swiss roll.\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax1.scatter(data[:, 0], data[:, 1], data[:, 2],\n",
    "                c=s_norm, cmap='viridis', s=5, alpha=0.7)\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "    ax1.set_title('Colored by s (angle)')\n",
    "\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.scatter(data[:, 0], data[:, 1], data[:, 2],\n",
    "                c=t_norm, cmap='plasma', s=5, alpha=0.7)\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.set_zlabel('Z')\n",
    "    ax2.set_title('Colored by t (height)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, \"swiss_roll_3d.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_latent_comparison(models, beta_values, data, s_norm, t_norm, plot_dir):\n",
    "    \"\"\"Compare latent spaces for different beta values\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    n_betas = len(beta_values)\n",
    "    fig, axes = plt.subplots(n_betas, 2, figsize=(10, 4 * n_betas))\n",
    "\n",
    "    if n_betas == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (model, beta) in enumerate(zip(models, beta_values)):\n",
    "        mu = get_latent_embeddings(model, data)\n",
    "\n",
    "        sc1 = axes[i, 0].scatter(mu[:, 0], mu[:, 1],\n",
    "                                 c=s_norm, cmap='viridis', s=3, alpha=0.7)\n",
    "        axes[i, 0].set_xlabel('$z_1$')\n",
    "        axes[i, 0].set_ylabel('$z_2$')\n",
    "        axes[i, 0].set_title(f'β={beta} (colored by s)')\n",
    "        axes[i, 0].set_aspect('equal', adjustable='box')\n",
    "        fig.colorbar(sc1, ax=axes[i, 0])\n",
    "\n",
    "        sc2 = axes[i, 1].scatter(mu[:, 0], mu[:, 1],\n",
    "                                 c=t_norm, cmap='plasma', s=3, alpha=0.7)\n",
    "        axes[i, 1].set_xlabel('$z_1$')\n",
    "        axes[i, 1].set_ylabel('$z_2$')\n",
    "        axes[i, 1].set_title(f'β={beta} (colored by t)')\n",
    "        axes[i, 1].set_aspect('equal', adjustable='box')\n",
    "        fig.colorbar(sc2, ax=axes[i, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, \"latent_comparison.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_reconstruction_quality(model, data, s_norm, beta, plot_dir):\n",
    "    \"\"\"Show reconstructed data for Swiss roll (3D), no original / prior samples.\"\"\"\n",
    "    ensure_plot_dir(plot_dir)\n",
    "    device = get_device()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(data).float().to(device)\n",
    "        # assuming model(x) returns [recons, input, mu, log_var]\n",
    "        x_recon, _, _, _ = model(x)\n",
    "        x_recon = x_recon.cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_recon[:, 0], x_recon[:, 1], x_recon[:, 2],\n",
    "               c=s_norm, cmap='viridis', s=3, alpha=0.7)\n",
    "    ax.set_title(f'Reconstructed (β={beta})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plot_dir, f\"reconstruction_beta_{beta}.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # still compute and print MSE vs original\n",
    "    mse = np.mean((data - x_recon) ** 2)\n",
    "\n",
    "# def plot_reconstruction_quality(model, data, s_norm, beta, plot_dir):\n",
    "#     \"\"\"Show original vs reconstructed data for Swiss roll.\"\"\"\n",
    "#     ensure_plot_dir(plot_dir)\n",
    "#     device = get_device()\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         x = torch.from_numpy(data).float().to(device)\n",
    "#         x_recon, _, _, _ = model(x)   # unpack 4 outputs\n",
    "#         x_recon = x_recon.cpu().numpy()\n",
    "\n",
    "#         z_prior = torch.randn(2000, model.latent_dim).to(device)\n",
    "#         x_samples = model.decode(z_prior).cpu().numpy()\n",
    "\n",
    "#     fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "#     ax1 = fig.add_subplot(131, projection='3d')\n",
    "#     ax1.scatter(data[:, 0], data[:, 1], data[:, 2],\n",
    "#                 c=s_norm, cmap='viridis', s=3, alpha=0.7)\n",
    "#     ax1.set_title('Original')\n",
    "\n",
    "#     ax2 = fig.add_subplot(132, projection='3d')\n",
    "#     ax2.scatter(x_recon[:, 0], x_recon[:, 1], x_recon[:, 2],\n",
    "#                 c=s_norm, cmap='viridis', s=3, alpha=0.7)\n",
    "#     ax2.set_title(f'Reconstructed (β={beta})')\n",
    "\n",
    "#     ax3 = fig.add_subplot(133, projection='3d')\n",
    "#     ax3.scatter(x_samples[:, 0], x_samples[:, 1], x_samples[:, 2],\n",
    "#                 c='gray', s=3, alpha=0.5)\n",
    "#     ax3.set_title('Prior Samples')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     fig.savefig(os.path.join(plot_dir, f\"reconstruction_beta_{beta}.png\"),\n",
    "#                 dpi=150, bbox_inches=\"tight\")\n",
    "#     plt.close(fig)\n",
    "\n",
    "#     mse = np.mean((data - x_recon) ** 2)\n",
    "#     print(f\"  β={beta}: MSE={mse:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_2d_experiment(name, data, colors, labels, config):\n",
    "    \"\"\"Run VAE experiment on a 2D dataset.\"\"\"\n",
    "    plot_dir = os.path.join(config[\"plot_dir\"], name)\n",
    "\n",
    "    # Plot original data\n",
    "    plot_2d_data(data, colors, labels, f\"{name} - Original\", plot_dir, \"original.png\")\n",
    "\n",
    "    # Train models across β\n",
    "    models, histories = _train_across_betas(data, config)\n",
    "\n",
    "    # Generate plots\n",
    "    plot_2d_latent_comparison(models, config[\"beta_values\"], data, colors, labels,\n",
    "                              plot_dir, name)\n",
    "    plot_2d_reconstruction(models, config[\"beta_values\"], data, colors, labels,\n",
    "                           plot_dir, name)\n",
    "    return models, histories\n",
    "\n",
    "\n",
    "def run_swiss_roll_experiment(config):\n",
    "    data, s_norm, t_norm, s, t = generate_swiss_roll(config[\"n_samples\"])\n",
    "\n",
    "    plot_dir = os.path.join(config[\"plot_dir\"], \"swiss_roll\")\n",
    "    plot_swiss_roll_3d(data, s_norm, t_norm, plot_dir)\n",
    "\n",
    "    # Train models across β\n",
    "    models, histories = _train_across_betas(data, config)\n",
    "\n",
    "    # Plots\n",
    "    plot_latent_comparison(models, config[\"beta_values\"], data, s_norm, t_norm, plot_dir)\n",
    "\n",
    "    for model, beta in zip(models, config[\"beta_values\"]):\n",
    "        plot_reconstruction_quality(model, data, s_norm, beta, plot_dir)\n",
    "\n",
    "    return models, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c162af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (4000, 3)\n",
      "  β=0.0: MSE=0.007276\n",
      "  β=0.5: MSE=0.008950\n",
      "  β=1.0: MSE=0.018183\n",
      "  β=5.0: MSE=0.060174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([SimpleVAE(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (fc_logvar): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (decoder_input): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       "  ),\n",
       "  SimpleVAE(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (fc_logvar): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (decoder_input): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       "  ),\n",
       "  SimpleVAE(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (fc_logvar): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (decoder_input): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       "  ),\n",
       "  SimpleVAE(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (fc_logvar): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (decoder_input): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (final_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )],\n",
       " [{'loss': [0.2806472202539444,\n",
       "    0.19086289811134338,\n",
       "    0.11464188712835312,\n",
       "    0.08608979877829552,\n",
       "    0.06828987342119217,\n",
       "    0.0529221998155117,\n",
       "    0.04061763986945152,\n",
       "    0.03128876978158951,\n",
       "    0.026351192489266397,\n",
       "    0.023807844758033752,\n",
       "    0.021866139456629755,\n",
       "    0.020533495858311653,\n",
       "    0.01885619916766882,\n",
       "    0.017477217212319374,\n",
       "    0.016442419044673442,\n",
       "    0.01570571404695511,\n",
       "    0.015210628911852837,\n",
       "    0.01505935886502266,\n",
       "    0.014607474982738495,\n",
       "    0.014992661759257316,\n",
       "    0.014059303246438504,\n",
       "    0.014104374721646308,\n",
       "    0.014311686277389527,\n",
       "    0.013810891687870025,\n",
       "    0.014232485458254814,\n",
       "    0.01385972848534584,\n",
       "    0.013405069380998611,\n",
       "    0.013253966532647609,\n",
       "    0.01360870673507452,\n",
       "    0.013001994542777538,\n",
       "    0.01275221335887909,\n",
       "    0.012620152227580547,\n",
       "    0.012232216268777848,\n",
       "    0.012106141552329064,\n",
       "    0.012591088078916074,\n",
       "    0.012794076085090637,\n",
       "    0.012358134292066097,\n",
       "    0.012152656756341458,\n",
       "    0.011269848611205816,\n",
       "    0.011312684185802937,\n",
       "    0.0112371431812644,\n",
       "    0.011133753888309002,\n",
       "    0.010686540111899376,\n",
       "    0.011088876601308584,\n",
       "    0.01115832718461752,\n",
       "    0.010359088554978371,\n",
       "    0.010044681139290332,\n",
       "    0.010353391498327254,\n",
       "    0.009675416871905327,\n",
       "    0.0096026774905622,\n",
       "    0.009486054055392741,\n",
       "    0.009289995439350605,\n",
       "    0.009464488156139851,\n",
       "    0.009658860333263875,\n",
       "    0.009072290167212487,\n",
       "    0.00915855684876442,\n",
       "    0.008681060306727887,\n",
       "    0.00840757567435503,\n",
       "    0.008371425725519657,\n",
       "    0.008133380331099033,\n",
       "    0.00827318025752902,\n",
       "    0.008037694673985242,\n",
       "    0.009092190574854612,\n",
       "    0.008491284057497979,\n",
       "    0.008960517950356007,\n",
       "    0.008081294033676386,\n",
       "    0.007609309263527393,\n",
       "    0.007495822802186012,\n",
       "    0.007906986933201552,\n",
       "    0.008173797361552715,\n",
       "    0.007284646093845368,\n",
       "    0.008048343546688557,\n",
       "    0.006945215243846178,\n",
       "    0.007011413540691137,\n",
       "    0.0070385984107851985,\n",
       "    0.007257640670984984,\n",
       "    0.007735974933952093,\n",
       "    0.007034636076539755,\n",
       "    0.007266921415925026,\n",
       "    0.0066857669912278655,\n",
       "    0.007201081153005362,\n",
       "    0.010205156050622463,\n",
       "    0.00817024826258421,\n",
       "    0.007883711349219084,\n",
       "    0.007205380234867335,\n",
       "    0.00645242078974843,\n",
       "    0.006971749182790518,\n",
       "    0.006394721111282706,\n",
       "    0.00655695891007781,\n",
       "    0.0067306368909776215,\n",
       "    0.00635348303988576,\n",
       "    0.00615002878010273,\n",
       "    0.006681896988302469,\n",
       "    0.007509593304246664,\n",
       "    0.0054291517212986945,\n",
       "    0.0075844131782650944,\n",
       "    0.006309005664661527,\n",
       "    0.007127276938408613,\n",
       "    0.006464335434138775,\n",
       "    0.006757547114044428],\n",
       "   'recon': [0.2806472202539444,\n",
       "    0.19086289811134338,\n",
       "    0.11464188712835312,\n",
       "    0.08608979877829552,\n",
       "    0.06828987342119217,\n",
       "    0.0529221998155117,\n",
       "    0.04061763986945152,\n",
       "    0.03128876978158951,\n",
       "    0.026351192489266397,\n",
       "    0.023807844758033752,\n",
       "    0.021866139456629755,\n",
       "    0.020533495858311653,\n",
       "    0.01885619916766882,\n",
       "    0.017477217212319374,\n",
       "    0.016442419044673442,\n",
       "    0.01570571404695511,\n",
       "    0.015210628911852837,\n",
       "    0.01505935886502266,\n",
       "    0.014607474982738495,\n",
       "    0.014992661759257316,\n",
       "    0.014059303246438504,\n",
       "    0.014104374721646308,\n",
       "    0.014311686277389527,\n",
       "    0.013810891687870025,\n",
       "    0.014232485458254814,\n",
       "    0.01385972848534584,\n",
       "    0.013405069380998611,\n",
       "    0.013253966532647609,\n",
       "    0.01360870673507452,\n",
       "    0.013001994542777538,\n",
       "    0.01275221335887909,\n",
       "    0.012620152227580547,\n",
       "    0.012232216268777848,\n",
       "    0.012106141552329064,\n",
       "    0.012591088078916074,\n",
       "    0.012794076085090637,\n",
       "    0.012358134292066097,\n",
       "    0.012152656756341458,\n",
       "    0.011269848611205816,\n",
       "    0.011312684185802937,\n",
       "    0.0112371431812644,\n",
       "    0.011133753888309002,\n",
       "    0.010686540111899376,\n",
       "    0.011088876601308584,\n",
       "    0.01115832718461752,\n",
       "    0.010359088554978371,\n",
       "    0.010044681139290332,\n",
       "    0.010353391498327254,\n",
       "    0.009675416871905327,\n",
       "    0.0096026774905622,\n",
       "    0.009486054055392741,\n",
       "    0.009289995439350605,\n",
       "    0.009464488156139851,\n",
       "    0.009658860333263875,\n",
       "    0.009072290167212487,\n",
       "    0.00915855684876442,\n",
       "    0.008681060306727887,\n",
       "    0.00840757567435503,\n",
       "    0.008371425725519657,\n",
       "    0.008133380331099033,\n",
       "    0.00827318025752902,\n",
       "    0.008037694673985242,\n",
       "    0.009092190574854612,\n",
       "    0.008491284057497979,\n",
       "    0.008960517950356007,\n",
       "    0.008081294033676386,\n",
       "    0.007609309263527393,\n",
       "    0.007495822802186012,\n",
       "    0.007906986933201552,\n",
       "    0.008173797361552715,\n",
       "    0.007284646093845368,\n",
       "    0.008048343546688557,\n",
       "    0.006945215243846178,\n",
       "    0.007011413540691137,\n",
       "    0.0070385984107851985,\n",
       "    0.007257640670984984,\n",
       "    0.007735974933952093,\n",
       "    0.007034636076539755,\n",
       "    0.007266921415925026,\n",
       "    0.0066857669912278655,\n",
       "    0.007201081153005362,\n",
       "    0.010205156050622463,\n",
       "    0.00817024826258421,\n",
       "    0.007883711349219084,\n",
       "    0.007205380234867335,\n",
       "    0.00645242078974843,\n",
       "    0.006971749182790518,\n",
       "    0.006394721111282706,\n",
       "    0.00655695891007781,\n",
       "    0.0067306368909776215,\n",
       "    0.00635348303988576,\n",
       "    0.00615002878010273,\n",
       "    0.006681896988302469,\n",
       "    0.007509593304246664,\n",
       "    0.0054291517212986945,\n",
       "    0.0075844131782650944,\n",
       "    0.006309005664661527,\n",
       "    0.007127276938408613,\n",
       "    0.006464335434138775,\n",
       "    0.006757547114044428],\n",
       "   'kld': [0.895909157820046,\n",
       "    4.632970211029053,\n",
       "    6.801873092651367,\n",
       "    8.84143825531006,\n",
       "    9.413917404174805,\n",
       "    9.16894467163086,\n",
       "    8.63251921081543,\n",
       "    8.486204879760741,\n",
       "    8.543226318359375,\n",
       "    8.509881721496582,\n",
       "    8.564821617126466,\n",
       "    8.666796096801757,\n",
       "    8.740492698669433,\n",
       "    8.773011573791504,\n",
       "    8.874846076965332,\n",
       "    8.950963676452636,\n",
       "    8.936688850402833,\n",
       "    9.065079444885255,\n",
       "    9.052180374145507,\n",
       "    9.126543624877929,\n",
       "    9.12914631652832,\n",
       "    9.13607893371582,\n",
       "    9.06925012588501,\n",
       "    9.083032585144043,\n",
       "    9.335034255981446,\n",
       "    8.996491653442384,\n",
       "    9.109740715026856,\n",
       "    9.026080207824707,\n",
       "    8.951009811401367,\n",
       "    8.985272590637207,\n",
       "    8.98105909729004,\n",
       "    8.943449531555176,\n",
       "    8.959396759033202,\n",
       "    8.980434623718262,\n",
       "    9.014276489257812,\n",
       "    9.092703926086426,\n",
       "    9.115454406738282,\n",
       "    9.112717964172363,\n",
       "    9.188492408752442,\n",
       "    9.20440737915039,\n",
       "    9.194349822998047,\n",
       "    9.23645152282715,\n",
       "    9.232457374572753,\n",
       "    9.274743881225586,\n",
       "    9.226025566101073,\n",
       "    9.162821640014648,\n",
       "    9.123081275939942,\n",
       "    9.120061096191407,\n",
       "    9.213809707641602,\n",
       "    9.248426261901855,\n",
       "    9.16423713684082,\n",
       "    9.297981956481934,\n",
       "    9.238266967773438,\n",
       "    9.222861324310303,\n",
       "    9.237408294677735,\n",
       "    9.146953872680664,\n",
       "    9.219481735229492,\n",
       "    9.24811344909668,\n",
       "    9.236777961730956,\n",
       "    9.281767242431641,\n",
       "    9.207471435546875,\n",
       "    9.129289222717285,\n",
       "    9.220005241394043,\n",
       "    9.115611129760742,\n",
       "    9.136281044006347,\n",
       "    9.281419799804688,\n",
       "    9.287558517456056,\n",
       "    9.429684356689453,\n",
       "    9.266954078674317,\n",
       "    9.520026840209962,\n",
       "    9.416060791015624,\n",
       "    9.264866622924805,\n",
       "    9.335620559692384,\n",
       "    9.406323249816895,\n",
       "    9.348464179992677,\n",
       "    9.44368334197998,\n",
       "    9.418466651916503,\n",
       "    9.670131469726563,\n",
       "    9.424167037963867,\n",
       "    9.17604271697998,\n",
       "    9.057605758666993,\n",
       "    8.660167358398438,\n",
       "    8.386238746643066,\n",
       "    9.387159881591797,\n",
       "    9.62585005569458,\n",
       "    9.444666206359864,\n",
       "    9.513681678771972,\n",
       "    9.549528770446777,\n",
       "    9.54440468597412,\n",
       "    9.053318565368652,\n",
       "    9.418462203979493,\n",
       "    9.17459098815918,\n",
       "    9.22806721496582,\n",
       "    9.189064468383789,\n",
       "    9.552163009643555,\n",
       "    9.148615745544433,\n",
       "    9.04908275604248,\n",
       "    9.004276329040527,\n",
       "    9.07992013168335,\n",
       "    9.218231063842774],\n",
       "   'beta': [0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0]},\n",
       "  {'loss': [0.2971610224246979,\n",
       "    0.22217694437503815,\n",
       "    0.1190648631453514,\n",
       "    0.09188909256458283,\n",
       "    0.08136758160591126,\n",
       "    0.07656064358353615,\n",
       "    0.06869706472754479,\n",
       "    0.0652128546833992,\n",
       "    0.06410402655601502,\n",
       "    0.06274979069828987,\n",
       "    0.06162430852651596,\n",
       "    0.058083295285701754,\n",
       "    0.05666253221035004,\n",
       "    0.055217578947544095,\n",
       "    0.05452728796005249,\n",
       "    0.05590826115012169,\n",
       "    0.05207180318236351,\n",
       "    0.05338776838779449,\n",
       "    0.052673516154289245,\n",
       "    0.05109523525834084,\n",
       "    0.05102747821807861,\n",
       "    0.04825444373488426,\n",
       "    0.04705952674150467,\n",
       "    0.046204018771648404,\n",
       "    0.04654833024740219,\n",
       "    0.047230865597724915,\n",
       "    0.04736260640621185,\n",
       "    0.04563900068402291,\n",
       "    0.04646185046434403,\n",
       "    0.046064428091049194,\n",
       "    0.04627587303519249,\n",
       "    0.04300399678945541,\n",
       "    0.044919165819883346,\n",
       "    0.044625611424446106,\n",
       "    0.04299076345562935,\n",
       "    0.04088897174596787,\n",
       "    0.04130204090476036,\n",
       "    0.03963427233695984,\n",
       "    0.041081872180104256,\n",
       "    0.04011286273598671,\n",
       "    0.03749716088175774,\n",
       "    0.03856201821565628,\n",
       "    0.041231782734394075,\n",
       "    0.03844227688014507,\n",
       "    0.03721901786327362,\n",
       "    0.03872726732492447,\n",
       "    0.03685607995092869,\n",
       "    0.036788252890110014,\n",
       "    0.03696366089582443,\n",
       "    0.03698949535191059,\n",
       "    0.03755755919218064,\n",
       "    0.037086369454860685,\n",
       "    0.03693862506747246,\n",
       "    0.03775701853632927,\n",
       "    0.03641749963164329,\n",
       "    0.03500977990031243,\n",
       "    0.03602344879508018,\n",
       "    0.035293540209531786,\n",
       "    0.036160969853401184,\n",
       "    0.0350602562725544,\n",
       "    0.034514128550887105,\n",
       "    0.034861651465296746,\n",
       "    0.03332075023651123,\n",
       "    0.03297668431699276,\n",
       "    0.03242498290538788,\n",
       "    0.032643092453479766,\n",
       "    0.0325317350924015,\n",
       "    0.03335767798125744,\n",
       "    0.03481917616724968,\n",
       "    0.03404347248375416,\n",
       "    0.037304072946310045,\n",
       "    0.03704331785440445,\n",
       "    0.033970313966274264,\n",
       "    0.03779099848866463,\n",
       "    0.03453721663355827,\n",
       "    0.03373907995223999,\n",
       "    0.03601428082585335,\n",
       "    0.0361018188148737,\n",
       "    0.03329192630946636,\n",
       "    0.03204480946063995,\n",
       "    0.03328696532547474,\n",
       "    0.03225082363188267,\n",
       "    0.03249148243665695,\n",
       "    0.03249454227089882,\n",
       "    0.03289025315642357,\n",
       "    0.0325378360748291,\n",
       "    0.03235288992524147,\n",
       "    0.03302570228278637,\n",
       "    0.031759347274899485,\n",
       "    0.032461018025875095,\n",
       "    0.031937273547053335,\n",
       "    0.03192704947292805,\n",
       "    0.03136584678292274,\n",
       "    0.03069606177508831,\n",
       "    0.0306473368704319,\n",
       "    0.030566080272197725,\n",
       "    0.03183525976538658,\n",
       "    0.03085604414343834,\n",
       "    0.031703251242637634,\n",
       "    0.031585277274250985],\n",
       "   'recon': [0.29337572228908537,\n",
       "    0.210064488530159,\n",
       "    0.10092588406801224,\n",
       "    0.07379349970817566,\n",
       "    0.062185864061117174,\n",
       "    0.05704671165347099,\n",
       "    0.048707653671503065,\n",
       "    0.044930037796497346,\n",
       "    0.04362498378753662,\n",
       "    0.042485183745622636,\n",
       "    0.04125559863448143,\n",
       "    0.037357480853796005,\n",
       "    0.036165902733802796,\n",
       "    0.03426130233705044,\n",
       "    0.033930774092674254,\n",
       "    0.03509424567222595,\n",
       "    0.03133765071630478,\n",
       "    0.0328957287222147,\n",
       "    0.032119406804442405,\n",
       "    0.03071130856871605,\n",
       "    0.030269837751984597,\n",
       "    0.027887104883790016,\n",
       "    0.026154345154762268,\n",
       "    0.025417238771915436,\n",
       "    0.02556181485950947,\n",
       "    0.026086105421185495,\n",
       "    0.026154244393110274,\n",
       "    0.024577436432242392,\n",
       "    0.02514386838674545,\n",
       "    0.02444471111893654,\n",
       "    0.0246875776425004,\n",
       "    0.021510054543614387,\n",
       "    0.023568575143814087,\n",
       "    0.02303327640891075,\n",
       "    0.021394050508737564,\n",
       "    0.019601251378655433,\n",
       "    0.019368432477116584,\n",
       "    0.017904930226504804,\n",
       "    0.01953899298608303,\n",
       "    0.018413675412535666,\n",
       "    0.016089220702648163,\n",
       "    0.0168359754383564,\n",
       "    0.019610890328884124,\n",
       "    0.016864072926342486,\n",
       "    0.015332207165658474,\n",
       "    0.016537808082997797,\n",
       "    0.015042423762381078,\n",
       "    0.014910092495381833,\n",
       "    0.014951120935380459,\n",
       "    0.015108761690557002,\n",
       "    0.015542858302593231,\n",
       "    0.015286397755146026,\n",
       "    0.014788212984800339,\n",
       "    0.015697491601109503,\n",
       "    0.014735025554895401,\n",
       "    0.01315653121471405,\n",
       "    0.014218260377645493,\n",
       "    0.013706685543060303,\n",
       "    0.014504059724509716,\n",
       "    0.013368583999574184,\n",
       "    0.012885268896818161,\n",
       "    0.013152562327682972,\n",
       "    0.011526776611804962,\n",
       "    0.011167051307857036,\n",
       "    0.010685479320585728,\n",
       "    0.0105068384334445,\n",
       "    0.010374395199120045,\n",
       "    0.011602634962648153,\n",
       "    0.012646403037011623,\n",
       "    0.01184549643099308,\n",
       "    0.014849768191576004,\n",
       "    0.014523891054093838,\n",
       "    0.011808117002248764,\n",
       "    0.015132819533348083,\n",
       "    0.012502088107168675,\n",
       "    0.011701978601515292,\n",
       "    0.01360221492499113,\n",
       "    0.013760426316410303,\n",
       "    0.011238273419439792,\n",
       "    0.010101395189762115,\n",
       "    0.011289068572223187,\n",
       "    0.010569527111947537,\n",
       "    0.010700927011668682,\n",
       "    0.010331953532993793,\n",
       "    0.011164577633142471,\n",
       "    0.010680490769445897,\n",
       "    0.010432558923959731,\n",
       "    0.010997213557362556,\n",
       "    0.009915114134550094,\n",
       "    0.010311562396585941,\n",
       "    0.009980978477746249,\n",
       "    0.010071827821433545,\n",
       "    0.00965119680762291,\n",
       "    0.008828225132077932,\n",
       "    0.00888050226122141,\n",
       "    0.008587554387748242,\n",
       "    0.010046747289597988,\n",
       "    0.009321442425251007,\n",
       "    0.009891858346760273,\n",
       "    0.009868012718856334],\n",
       "   'kld': [0.479883556291461,\n",
       "    1.5212589988708496,\n",
       "    2.2755987396240234,\n",
       "    2.271898235321045,\n",
       "    2.407684223175049,\n",
       "    2.4491861839294433,\n",
       "    2.508542701721191,\n",
       "    2.545361721038818,\n",
       "    2.5700987377166746,\n",
       "    2.54320813369751,\n",
       "    2.55610954284668,\n",
       "    2.6004756927490233,\n",
       "    2.572277572631836,\n",
       "    2.6299275970458984,\n",
       "    2.5850849018096924,\n",
       "    2.6117181091308592,\n",
       "    2.602171630859375,\n",
       "    2.5718058204650878,\n",
       "    2.579229362487793,\n",
       "    2.558787240982056,\n",
       "    2.6050639476776123,\n",
       "    2.5566185150146485,\n",
       "    2.6235082550048827,\n",
       "    2.6094800109863283,\n",
       "    2.634282695770264,\n",
       "    2.6539090423583986,\n",
       "    2.661603946685791,\n",
       "    2.6437877044677736,\n",
       "    2.6750783462524415,\n",
       "    2.7137091884613036,\n",
       "    2.7095379867553713,\n",
       "    2.6971256923675537,\n",
       "    2.679780666351318,\n",
       "    2.7095622539520265,\n",
       "    2.710132387161255,\n",
       "    2.6721380233764647,\n",
       "    2.7526395435333253,\n",
       "    2.7269326667785645,\n",
       "    2.703939926147461,\n",
       "    2.7227148132324217,\n",
       "    2.6869840469360353,\n",
       "    2.726759410858154,\n",
       "    2.713079902648926,\n",
       "    2.7085191555023194,\n",
       "    2.7472049179077147,\n",
       "    2.7851421012878417,\n",
       "    2.737189163208008,\n",
       "    2.745629446029663,\n",
       "    2.762575836181641,\n",
       "    2.745349754333496,\n",
       "    2.7630166454315184,\n",
       "    2.7361564769744873,\n",
       "    2.7799385452270506,\n",
       "    2.768133041381836,\n",
       "    2.721885238647461,\n",
       "    2.742183723449707,\n",
       "    2.736522361755371,\n",
       "    2.709003980636597,\n",
       "    2.7176634941101074,\n",
       "    2.7226332035064695,\n",
       "    2.714544979095459,\n",
       "    2.724479965209961,\n",
       "    2.7352960739135743,\n",
       "    2.7370105800628663,\n",
       "    2.728514945983887,\n",
       "    2.777639305114746,\n",
       "    2.7805096969604493,\n",
       "    2.7302130699157714,\n",
       "    2.782957981109619,\n",
       "    2.785927640914917,\n",
       "    2.8172414360046387,\n",
       "    2.826154155731201,\n",
       "    2.7816444149017334,\n",
       "    2.8440377883911134,\n",
       "    2.7652714614868166,\n",
       "    2.765782844543457,\n",
       "    2.812460844039917,\n",
       "    2.803707067489624,\n",
       "    2.767319969177246,\n",
       "    2.7535778694152833,\n",
       "    2.7607243347167967,\n",
       "    2.721365526199341,\n",
       "    2.7345076484680177,\n",
       "    2.781101993560791,\n",
       "    2.726627576828003,\n",
       "    2.7436864604949953,\n",
       "    2.7509852924346925,\n",
       "    2.764315002441406,\n",
       "    2.7414264335632326,\n",
       "    2.7792867355346678,\n",
       "    2.7561809310913086,\n",
       "    2.742530746459961,\n",
       "    2.725153854370117,\n",
       "    2.743984327316284,\n",
       "    2.731363260269165,\n",
       "    2.757943817138672,\n",
       "    2.7343097705841064,\n",
       "    2.702793388366699,\n",
       "    2.737573450088501,\n",
       "    2.7255406532287596],\n",
       "   'beta': [0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5]},\n",
       "  {'loss': [0.31155649256706236,\n",
       "    0.21857171583175658,\n",
       "    0.14037739157676696,\n",
       "    0.1085158063173294,\n",
       "    0.08369114291667938,\n",
       "    0.07467783379554749,\n",
       "    0.06874984908103943,\n",
       "    0.06433559340238572,\n",
       "    0.06079754257202148,\n",
       "    0.05975720137357712,\n",
       "    0.059071219384670255,\n",
       "    0.05777041536569595,\n",
       "    0.058598509311676025,\n",
       "    0.05792857363820076,\n",
       "    0.057975124180316924,\n",
       "    0.05846601408720017,\n",
       "    0.05807586467266083,\n",
       "    0.05744764673709869,\n",
       "    0.05712359893321991,\n",
       "    0.057902654439210895,\n",
       "    0.05787312030792236,\n",
       "    0.057043306350708006,\n",
       "    0.05796275103092194,\n",
       "    0.05646208980679512,\n",
       "    0.056980844855308535,\n",
       "    0.05652589559555054,\n",
       "    0.05694050794839859,\n",
       "    0.055814198940992356,\n",
       "    0.056452235400676724,\n",
       "    0.05614215761423111,\n",
       "    0.05594881939888,\n",
       "    0.055644403666257856,\n",
       "    0.056177773386240006,\n",
       "    0.05578005278110504,\n",
       "    0.0554199059009552,\n",
       "    0.0557464502453804,\n",
       "    0.05589105781912804,\n",
       "    0.05550081253051758,\n",
       "    0.05544332814216614,\n",
       "    0.056083429992198945,\n",
       "    0.05582780170440674,\n",
       "    0.05702152079343796,\n",
       "    0.05538199496269226,\n",
       "    0.05535609251260758,\n",
       "    0.056049582302570344,\n",
       "    0.05574221086502075,\n",
       "    0.05484382098913193,\n",
       "    0.05507061630487442,\n",
       "    0.055257712423801425,\n",
       "    0.05517983055114746,\n",
       "    0.05470072513818741,\n",
       "    0.05505522570014,\n",
       "    0.05493634563684464,\n",
       "    0.055128662526607516,\n",
       "    0.05473254895210266,\n",
       "    0.05495989149808884,\n",
       "    0.054373353153467176,\n",
       "    0.05501355439424515,\n",
       "    0.05448151585459709,\n",
       "    0.05425571286678314,\n",
       "    0.05430365240573883,\n",
       "    0.05374494495987892,\n",
       "    0.05508821171522141,\n",
       "    0.054358294069767,\n",
       "    0.05428350964188576,\n",
       "    0.05426820412278175,\n",
       "    0.05415428197383881,\n",
       "    0.05394566512107849,\n",
       "    0.05344642508029938,\n",
       "    0.054136624693870546,\n",
       "    0.05358464902639389,\n",
       "    0.053746288865804674,\n",
       "    0.053901045113801956,\n",
       "    0.053159951359033585,\n",
       "    0.053417214035987856,\n",
       "    0.05316331696510315,\n",
       "    0.05317566335201263,\n",
       "    0.052822310984134674,\n",
       "    0.05307228773832321,\n",
       "    0.05314662736654282,\n",
       "    0.05262195309996605,\n",
       "    0.05307102975249291,\n",
       "    0.05235591718554497,\n",
       "    0.05298124629259109,\n",
       "    0.05305821788311005,\n",
       "    0.05284900140762329,\n",
       "    0.05289173613488674,\n",
       "    0.05293119165301323,\n",
       "    0.05213314589858055,\n",
       "    0.052478698551654816,\n",
       "    0.05236414721608162,\n",
       "    0.05164807844161987,\n",
       "    0.05246631866693497,\n",
       "    0.052278447806835174,\n",
       "    0.05220549875497818,\n",
       "    0.05285633212327957,\n",
       "    0.05236657625436783,\n",
       "    0.05158335435390472,\n",
       "    0.05207785269618034,\n",
       "    0.05207952779531479],\n",
       "   'recon': [0.3084580632448196,\n",
       "    0.20276161801815032,\n",
       "    0.11402646416425705,\n",
       "    0.07943393403291703,\n",
       "    0.05213801369071007,\n",
       "    0.04265644907951355,\n",
       "    0.03613176880776882,\n",
       "    0.03141174095869064,\n",
       "    0.028084380432963373,\n",
       "    0.026506063356995582,\n",
       "    0.02629388938844204,\n",
       "    0.025381585076451302,\n",
       "    0.025726001918315887,\n",
       "    0.025082250103354454,\n",
       "    0.025162043899297715,\n",
       "    0.02589016281068325,\n",
       "    0.025244549736380576,\n",
       "    0.024207955852150916,\n",
       "    0.024473798289895057,\n",
       "    0.025054975405335425,\n",
       "    0.02488035672903061,\n",
       "    0.024172344788908957,\n",
       "    0.025164689525961875,\n",
       "    0.023542478948831557,\n",
       "    0.02417086498439312,\n",
       "    0.02379684340953827,\n",
       "    0.023985882952809335,\n",
       "    0.023250530228018762,\n",
       "    0.023728472664952278,\n",
       "    0.023439853355288507,\n",
       "    0.02311203594505787,\n",
       "    0.023070402964949607,\n",
       "    0.02330899877846241,\n",
       "    0.022608469769358635,\n",
       "    0.02288756662607193,\n",
       "    0.02307570157945156,\n",
       "    0.023090065240859985,\n",
       "    0.02301155512034893,\n",
       "    0.022664074659347534,\n",
       "    0.02367755727469921,\n",
       "    0.02299807059764862,\n",
       "    0.024095053344964982,\n",
       "    0.02264699298143387,\n",
       "    0.022478994324803353,\n",
       "    0.022964783877134323,\n",
       "    0.02258568699657917,\n",
       "    0.021796799808740615,\n",
       "    0.022041129156947135,\n",
       "    0.022438997730612757,\n",
       "    0.022081190332770347,\n",
       "    0.021571615859866142,\n",
       "    0.02208422675728798,\n",
       "    0.02171885660290718,\n",
       "    0.0221512403935194,\n",
       "    0.021938161581754684,\n",
       "    0.021956668794155122,\n",
       "    0.021274046018719673,\n",
       "    0.021750440254807473,\n",
       "    0.02142973844707012,\n",
       "    0.021059263542294503,\n",
       "    0.021121124431490897,\n",
       "    0.02067285007238388,\n",
       "    0.02196951822936535,\n",
       "    0.02130144192278385,\n",
       "    0.021019099354743956,\n",
       "    0.020929761186242103,\n",
       "    0.020428871005773543,\n",
       "    0.020499928385019302,\n",
       "    0.02009017077088356,\n",
       "    0.02048280020058155,\n",
       "    0.019838379964232444,\n",
       "    0.020455723360180856,\n",
       "    0.02011193612217903,\n",
       "    0.019746171943843364,\n",
       "    0.019688562035560606,\n",
       "    0.019615651667118074,\n",
       "    0.01925944943726063,\n",
       "    0.019347330555319786,\n",
       "    0.019426641911268236,\n",
       "    0.01923277209699154,\n",
       "    0.0190368250310421,\n",
       "    0.019169226095080375,\n",
       "    0.018573620468378068,\n",
       "    0.01887842246890068,\n",
       "    0.0191385730355978,\n",
       "    0.01875435048341751,\n",
       "    0.019051985561847687,\n",
       "    0.018956602737307548,\n",
       "    0.01789563062787056,\n",
       "    0.01832092286646366,\n",
       "    0.01829445493221283,\n",
       "    0.017442263051867486,\n",
       "    0.01844466896355152,\n",
       "    0.018003029927611353,\n",
       "    0.017879337519407273,\n",
       "    0.018507294371724128,\n",
       "    0.017778438791632652,\n",
       "    0.017184764444828034,\n",
       "    0.017588221795856954,\n",
       "    0.017793721362948416],\n",
       "   'kld': [0.19850615917891265,\n",
       "    0.9935485935211181,\n",
       "    1.654129545211792,\n",
       "    1.8252224073410035,\n",
       "    1.979932993888855,\n",
       "    2.0098910579681397,\n",
       "    2.0467710552215577,\n",
       "    2.0656649465560912,\n",
       "    2.0528875789642336,\n",
       "    2.0864784927368163,\n",
       "    2.056044136047363,\n",
       "    2.0327599468231203,\n",
       "    2.0626586475372313,\n",
       "    2.0607440090179443,\n",
       "    2.0589705715179445,\n",
       "    2.044356300354004,\n",
       "    2.0601186389923094,\n",
       "    2.0856263904571533,\n",
       "    2.049084218978882,\n",
       "    2.0612668800354004,\n",
       "    2.0706959743499755,\n",
       "    2.062980766296387,\n",
       "    2.0578829975128174,\n",
       "    2.065796899795532,\n",
       "    2.0587439861297607,\n",
       "    2.053967357635498,\n",
       "    2.0678257522583006,\n",
       "    2.0433799743652346,\n",
       "    2.0532772102355956,\n",
       "    2.052059970855713,\n",
       "    2.0603104095458984,\n",
       "    2.0439461078643797,\n",
       "    2.0625755290985106,\n",
       "    2.0810607137680055,\n",
       "    2.041483211517334,\n",
       "    2.049868095397949,\n",
       "    2.0583765659332274,\n",
       "    2.038773027420044,\n",
       "    2.0571634426116945,\n",
       "    2.0335726852416993,\n",
       "    2.059922208786011,\n",
       "    2.0662493228912355,\n",
       "    2.0541240577697755,\n",
       "    2.063043758392334,\n",
       "    2.0758507614135744,\n",
       "    2.0805948543548585,\n",
       "    2.073615238189697,\n",
       "    2.0729799728393554,\n",
       "    2.059478698730469,\n",
       "    2.07705460357666,\n",
       "    2.0783640632629394,\n",
       "    2.068426300048828,\n",
       "    2.084309507369995,\n",
       "    2.06931183052063,\n",
       "    2.057932771682739,\n",
       "    2.0707848091125487,\n",
       "    2.0770874500274656,\n",
       "    2.0873101863861083,\n",
       "    2.073979999542236,\n",
       "    2.0828514728546144,\n",
       "    2.082203758239746,\n",
       "    2.075735824584961,\n",
       "    2.07833687210083,\n",
       "    2.074316005706787,\n",
       "    2.087043773651123,\n",
       "    2.0924343948364257,\n",
       "    2.1162692413330078,\n",
       "    2.0988788566589354,\n",
       "    2.0930030555725097,\n",
       "    2.1116031589508055,\n",
       "    2.1175799674987794,\n",
       "    2.0886360473632815,\n",
       "    2.119950050354004,\n",
       "    2.0966373710632324,\n",
       "    2.1163151931762694,\n",
       "    2.1049024658203126,\n",
       "    2.127917381286621,\n",
       "    2.100736066818237,\n",
       "    2.1112991847991944,\n",
       "    2.1279678974151612,\n",
       "    2.1073548622131346,\n",
       "    2.1275600128173826,\n",
       "    2.1195594310760497,\n",
       "    2.1400460014343263,\n",
       "    2.12842741394043,\n",
       "    2.139640769958496,\n",
       "    2.1234309902191164,\n",
       "    2.1320589485168457,\n",
       "    2.148264457702637,\n",
       "    2.1434778327941895,\n",
       "    2.1376934356689454,\n",
       "    2.146414726257324,\n",
       "    2.134968200683594,\n",
       "    2.1506536865234374,\n",
       "    2.154179096221924,\n",
       "    2.1555317306518553,\n",
       "    2.1703785972595213,\n",
       "    2.1585179138183594,\n",
       "    2.1642958545684814,\n",
       "    2.151436138153076],\n",
       "   'beta': [1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0]},\n",
       "  {'loss': [0.3091403958797455,\n",
       "    0.2808997800350189,\n",
       "    0.2468394193649292,\n",
       "    0.21471173417568207,\n",
       "    0.19797213482856751,\n",
       "    0.1883474814891815,\n",
       "    0.17671270394325256,\n",
       "    0.1728410793542862,\n",
       "    0.16691101723909377,\n",
       "    0.16075139033794403,\n",
       "    0.15995877504348754,\n",
       "    0.15831251859664916,\n",
       "    0.15740559947490693,\n",
       "    0.15814080953598023,\n",
       "    0.157194360435009,\n",
       "    0.15705388677120208,\n",
       "    0.15476394510269165,\n",
       "    0.15473343271017076,\n",
       "    0.15503394055366515,\n",
       "    0.15537436217069625,\n",
       "    0.15675823783874512,\n",
       "    0.15632698094844819,\n",
       "    0.15356434917449951,\n",
       "    0.15432362049818038,\n",
       "    0.1532390090227127,\n",
       "    0.15535870230197907,\n",
       "    0.1545943866968155,\n",
       "    0.15468579947948455,\n",
       "    0.15525473964214326,\n",
       "    0.1547272971868515,\n",
       "    0.15293721866607665,\n",
       "    0.15354443389177322,\n",
       "    0.15436345303058624,\n",
       "    0.15522274446487427,\n",
       "    0.15433396250009537,\n",
       "    0.1543712220788002,\n",
       "    0.15622463023662567,\n",
       "    0.15518906545639038,\n",
       "    0.1541187583208084,\n",
       "    0.15354719150066376,\n",
       "    0.15352542912960052,\n",
       "    0.15287154722213744,\n",
       "    0.1526585111618042,\n",
       "    0.15321325194835664,\n",
       "    0.15323700141906738,\n",
       "    0.15419719886779784,\n",
       "    0.15415491688251495,\n",
       "    0.15330315852165222,\n",
       "    0.15343160325288774,\n",
       "    0.15293278527259827,\n",
       "    0.1530479016304016,\n",
       "    0.1529005103111267,\n",
       "    0.15321547770500182,\n",
       "    0.15564128422737122,\n",
       "    0.15469992864131926,\n",
       "    0.15353554010391235,\n",
       "    0.153461895942688,\n",
       "    0.15297885072231293,\n",
       "    0.15313480842113494,\n",
       "    0.15410745394229888,\n",
       "    0.15299397099018097,\n",
       "    0.15475708001852034,\n",
       "    0.15394299042224885,\n",
       "    0.15224350774288178,\n",
       "    0.15384689962863923,\n",
       "    0.15220091086626053,\n",
       "    0.15341822838783264,\n",
       "    0.15391528517007827,\n",
       "    0.15427617210149766,\n",
       "    0.15133784550428392,\n",
       "    0.15450132471323014,\n",
       "    0.1539817509651184,\n",
       "    0.15394636619091034,\n",
       "    0.15327604615688323,\n",
       "    0.1535699136853218,\n",
       "    0.1527523133158684,\n",
       "    0.15317186015844345,\n",
       "    0.15377833378314973,\n",
       "    0.15375278669595718,\n",
       "    0.1535474797487259,\n",
       "    0.15411769676208495,\n",
       "    0.15233128321170808,\n",
       "    0.15222445142269134,\n",
       "    0.15205921518802643,\n",
       "    0.15339043074846268,\n",
       "    0.1523821177482605,\n",
       "    0.1534505063891411,\n",
       "    0.15273389840126037,\n",
       "    0.15347425496578215,\n",
       "    0.15374366056919098,\n",
       "    0.1534934848546982,\n",
       "    0.15274880540370941,\n",
       "    0.15473900306224822,\n",
       "    0.15365644240379334,\n",
       "    0.1529515095949173,\n",
       "    0.15319331103563308,\n",
       "    0.15341504806280137,\n",
       "    0.1537148631811142,\n",
       "    0.1538242987394333,\n",
       "    0.15277263247966766],\n",
       "   'recon': [0.2943360117673874,\n",
       "    0.24614424920082092,\n",
       "    0.19793778049945832,\n",
       "    0.14743944036960602,\n",
       "    0.12307786577939987,\n",
       "    0.10704246914386749,\n",
       "    0.09028734123706818,\n",
       "    0.08516641736030578,\n",
       "    0.07480005472898484,\n",
       "    0.06953247678279877,\n",
       "    0.06558722692728043,\n",
       "    0.06287196722626687,\n",
       "    0.06334390556812286,\n",
       "    0.0627476897239685,\n",
       "    0.062039237678050994,\n",
       "    0.06269292443990708,\n",
       "    0.06100916701555252,\n",
       "    0.060396968066692355,\n",
       "    0.0598950280547142,\n",
       "    0.06080672949552536,\n",
       "    0.06188370984792709,\n",
       "    0.062109581112861634,\n",
       "    0.05981375333666802,\n",
       "    0.05903889647126198,\n",
       "    0.05930409336090088,\n",
       "    0.06000417047739029,\n",
       "    0.05910478779673577,\n",
       "    0.06055749982595444,\n",
       "    0.05905136263370514,\n",
       "    0.05896036878228188,\n",
       "    0.05905696105957031,\n",
       "    0.059305145531892776,\n",
       "    0.05943042284250259,\n",
       "    0.06044150426983833,\n",
       "    0.05961214208602905,\n",
       "    0.06056517022848129,\n",
       "    0.05926725190877914,\n",
       "    0.059563802540302274,\n",
       "    0.05876891967654228,\n",
       "    0.059379773139953615,\n",
       "    0.0583794584274292,\n",
       "    0.05819353127479553,\n",
       "    0.05823479041457176,\n",
       "    0.05881623044610024,\n",
       "    0.05792894786596298,\n",
       "    0.05929487997293472,\n",
       "    0.05890049934387207,\n",
       "    0.05940442568063736,\n",
       "    0.05736964595317841,\n",
       "    0.05863382747769356,\n",
       "    0.05885759407281876,\n",
       "    0.05824431735277176,\n",
       "    0.05874333044886589,\n",
       "    0.05949714434146881,\n",
       "    0.06019722482562065,\n",
       "    0.05869406795501709,\n",
       "    0.059232379645109176,\n",
       "    0.057791536420583725,\n",
       "    0.05764253097772598,\n",
       "    0.05850481957197189,\n",
       "    0.05734300723671913,\n",
       "    0.05935929587483406,\n",
       "    0.056955198407173155,\n",
       "    0.059017260491847995,\n",
       "    0.0585937722325325,\n",
       "    0.05783972781896591,\n",
       "    0.0588787767291069,\n",
       "    0.05780779603123665,\n",
       "    0.05890365928411484,\n",
       "    0.05739109644293785,\n",
       "    0.060440374314785,\n",
       "    0.05735537904500961,\n",
       "    0.057769649147987365,\n",
       "    0.05729911190271378,\n",
       "    0.057000384241342544,\n",
       "    0.057354422807693484,\n",
       "    0.05842981445789337,\n",
       "    0.05653943336009979,\n",
       "    0.057701545417308805,\n",
       "    0.05784777656197548,\n",
       "    0.05785061839222908,\n",
       "    0.057095337986946104,\n",
       "    0.05806614431738853,\n",
       "    0.05735860627889633,\n",
       "    0.05774602895975113,\n",
       "    0.05765766802430153,\n",
       "    0.05766766503453255,\n",
       "    0.05873898822069168,\n",
       "    0.058178491055965426,\n",
       "    0.05798322480916977,\n",
       "    0.05893120715022087,\n",
       "    0.05707286873459816,\n",
       "    0.05861023825407028,\n",
       "    0.058539445787668225,\n",
       "    0.05702667045593262,\n",
       "    0.05687358084321022,\n",
       "    0.057595511317253115,\n",
       "    0.057343403458595274,\n",
       "    0.05759684216976166,\n",
       "    0.057573919743299486],\n",
       "   'kld': [0.18734258637204768,\n",
       "    0.4363293762207031,\n",
       "    0.6142821550369263,\n",
       "    0.8443273096084595,\n",
       "    0.9397718071937561,\n",
       "    1.0204386596679687,\n",
       "    1.0844993238449097,\n",
       "    1.1004352388381957,\n",
       "    1.155954273223877,\n",
       "    1.144944580078125,\n",
       "    1.1843245124816895,\n",
       "    1.197864761352539,\n",
       "    1.1806626415252686,\n",
       "    1.1973396701812744,\n",
       "    1.1943368492126465,\n",
       "    1.1843880081176759,\n",
       "    1.1766817445755005,\n",
       "    1.184234094619751,\n",
       "    1.1934993438720702,\n",
       "    1.1868388118743896,\n",
       "    1.1907638292312621,\n",
       "    1.1823156242370605,\n",
       "    1.1765111770629884,\n",
       "    1.1955871801376343,\n",
       "    1.1786601829528809,\n",
       "    1.1965760889053345,\n",
       "    1.1983410863876343,\n",
       "    1.1816995344161987,\n",
       "    1.2072842025756836,\n",
       "    1.2013579559326173,\n",
       "    1.177923846244812,\n",
       "    1.1829058446884155,\n",
       "    1.1913708515167236,\n",
       "    1.1899167938232422,\n",
       "    1.188275182723999,\n",
       "    1.1771744270324707,\n",
       "    1.2170185661315918,\n",
       "    1.1999370956420898,\n",
       "    1.1966607961654663,\n",
       "    1.1818767671585082,\n",
       "    1.194024871826172,\n",
       "    1.188039171218872,\n",
       "    1.1847988739013673,\n",
       "    1.1849247465133668,\n",
       "    1.1963488588333129,\n",
       "    1.190998028755188,\n",
       "    1.195249608039856,\n",
       "    1.1782726078033448,\n",
       "    1.2057991962432861,\n",
       "    1.1833158473968506,\n",
       "    1.1824200172424317,\n",
       "    1.18813924407959,\n",
       "    1.18544832611084,\n",
       "    1.2063283233642579,\n",
       "    1.186027590751648,\n",
       "    1.1903160028457642,\n",
       "    1.1828092346191406,\n",
       "    1.1943854007720947,\n",
       "    1.1983844118118285,\n",
       "    1.199654685974121,\n",
       "    1.200451150894165,\n",
       "    1.1975491285324096,\n",
       "    1.2169494676589965,\n",
       "    1.1701225452423096,\n",
       "    1.1955739374160768,\n",
       "    1.1839360637664795,\n",
       "    1.1862244262695312,\n",
       "    1.2063185386657715,\n",
       "    1.1971764640808105,\n",
       "    1.1788276329040528,\n",
       "    1.1802583675384521,\n",
       "    1.2127422370910645,\n",
       "    1.2070210695266723,\n",
       "    1.2046903533935547,\n",
       "    1.2118422021865844,\n",
       "    1.1971196985244752,\n",
       "    1.1892872524261475,\n",
       "    1.220234676361084,\n",
       "    1.2049582252502442,\n",
       "    1.2013162813186646,\n",
       "    1.2084025115966797,\n",
       "    1.195467290878296,\n",
       "    1.1814642152786254,\n",
       "    1.188358980178833,\n",
       "    1.2001523933410645,\n",
       "    1.1890371408462523,\n",
       "    1.2019529056549072,\n",
       "    1.1797594528198243,\n",
       "    1.1955361814498902,\n",
       "    1.201982674598694,\n",
       "    1.1864908056259156,\n",
       "    1.2009541873931884,\n",
       "    1.20600537109375,\n",
       "    1.1939209270477296,\n",
       "    1.2038302192687989,\n",
       "    1.2092288455963134,\n",
       "    1.2024112472534179,\n",
       "    1.2092051458358766,\n",
       "    1.2075314540863038,\n",
       "    1.1948786029815675],\n",
       "   'beta': [5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0,\n",
       "    5.0]}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# 2D datasets\n",
    "data, colors, theta = generate_single_circle(CONFIG[\"n_samples\"])\n",
    "run_2d_experiment(\"single_circle\", data, colors, None, CONFIG)\n",
    "\n",
    "data, colors, labels = generate_three_circles(CONFIG[\"n_samples\"])\n",
    "run_2d_experiment(\"three_circles\", data, colors, labels, CONFIG)\n",
    "\n",
    "data, colors, labels = generate_smile_face(CONFIG[\"n_samples\"])\n",
    "run_2d_experiment(\"smile_face\", data, colors, labels, CONFIG)\n",
    "\n",
    "# 3D dataset\n",
    "run_swiss_roll_experiment(CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
