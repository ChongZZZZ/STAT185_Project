{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aaec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vanilla_vae import VanillaVAE\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bba982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minst dataset loader\n",
    "def get_mnist_loaders(batch_size=64):\n",
    "    \"\"\"\n",
    "    Train on digits 0-8 only (normal). Test on all digits (0-9).\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),                     \n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # (3, H, W) \n",
    "    ])\n",
    "\n",
    "    train_full = datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_full = datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Train only on digits 0-8 (normal data)\n",
    "    normal_indices = [i for i, (_, y) in enumerate(train_full) if y != 9]\n",
    "    train_normal = Subset(train_full, normal_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_normal, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_full, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Gradient Descent Trainning\n",
    "def train_mnist_vae(latent_dim=128,batch_size=64, num_epochs=20, lr=0.005, device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size=batch_size)\n",
    "\n",
    "    model = VanillaVAE(in_channels=3, latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_recon = 0.0\n",
    "        total_kld = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            recons, x_in, mu, log_var = model(x)\n",
    "            M_N = x.size(0) / len(train_loader.dataset) \n",
    "            loss_dict = model.loss_function(\n",
    "                recons, x_in, mu, log_var, M_N=M_N\n",
    "            )\n",
    "            loss = loss_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_recon += loss_dict[\"Reconstruction_Loss\"].item() * x.size(0)\n",
    "            total_kld += loss_dict[\"KLD\"].item() * x.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        avg_recon = total_recon / len(train_loader.dataset)\n",
    "        avg_kld = total_kld / len(train_loader.dataset)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return model, test_loader, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Model (The model is trained on Google Colab)\n",
    "model, test_loader, device = train_mnist_vae(latent_dim=128,batch_size=64, num_epochs=20, lr=0.005, device=None)\n",
    "torch.save(model.state_dict(), \"mnist_vae_vanilla.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = VanillaVAE(in_channels=3, latent_dim=128).to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_vae_vanilla.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Get test data\n",
    "transform = transforms.Compose([transforms.Resize(64),transforms.ToTensor(),transforms.Lambda(lambda x: x.repeat(3, 1, 1))])\n",
    "test_full = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_full, batch_size=64, shuffle=False)\n",
    "\n",
    "# Compute errors\n",
    "def compute_reconstruction_errors(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_errors = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            recons, x_in, mu, log_var = model(x)\n",
    "            mse = torch.mean((recons - x_in) ** 2, dim=(1, 2, 3)) # L2 difference of actual and generated\n",
    "            all_errors.append(mse.cpu().numpy())\n",
    "            all_labels.append(y.numpy())\n",
    "    errors = np.concatenate(all_errors)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    return errors, labels\n",
    "\n",
    "# Plot histogram\n",
    "def plot_anomaly_histogram(errors, labels):\n",
    "    normal_mask = labels != 9\n",
    "    anomaly_mask = labels == 9\n",
    "    normal_errors = errors[normal_mask]\n",
    "    anomaly_errors = errors[anomaly_mask]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Histograms density\n",
    "    ax.hist(normal_errors, bins=50, alpha=0.4, label=\"Digits 0-8 (normal)\",\n",
    "            density=True, color='steelblue')\n",
    "    ax.hist(anomaly_errors, bins=50, alpha=0.4, label=\"Digit 9 (anomaly)\",\n",
    "            density=True, color='coral')\n",
    "\n",
    "    ax.set_xlabel(\"Reconstruction MSE\", fontsize=12)\n",
    "    ax.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title(\"MNIST VAE Reconstruction Errors\", fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('anomaly_histogram.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show reconstructions \n",
    "def show_reconstructions(model, data_loader, device, num_examples=8):\n",
    "    model.eval()\n",
    "\n",
    "    # Take a single batch\n",
    "    x_batch, y_batch = next(iter(data_loader))\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    # Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        recons = model.generate(x_batch)\n",
    "\n",
    "    def select_indices(mask, k):\n",
    "        return mask.nonzero(as_tuple=True)[0][:k]\n",
    "\n",
    "    # Normal digits: 0â€“8, anomalies: 9\n",
    "    normal_idx = select_indices(y_batch != 9, num_examples)\n",
    "    anomaly_idx = select_indices(y_batch == 9, num_examples)\n",
    "\n",
    "    def to_image(t):\n",
    "        img = t.detach().cpu().permute(1, 2, 0)  \n",
    "        img = (img + 1.0) / 2.0                  \n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        return img.numpy()\n",
    "\n",
    "    def plot_pairs(indices, title):\n",
    "        if len(indices) == 0:\n",
    "            return\n",
    "\n",
    "        n = len(indices)\n",
    "        fig, axes = plt.subplots(2, n, figsize=(2 * n, 4))\n",
    "\n",
    "        # If n == 1, axes will not be 2D by default\n",
    "        if n == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "\n",
    "        for col, idx in enumerate(indices):\n",
    "            orig_img = to_image(x_batch[idx])\n",
    "            rec_img = to_image(recons[idx])\n",
    "\n",
    "            axes[0, col].imshow(orig_img, cmap=\"gray\")\n",
    "            axes[0, col].axis(\"off\")\n",
    "\n",
    "            axes[1, col].imshow(rec_img, cmap=\"gray\")\n",
    "            axes[1, col].axis(\"off\")\n",
    "\n",
    "        axes[0, 0].set_title(f\"{title} - original\")\n",
    "        axes[1, 0].set_title(f\"{title} - reconstruction\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        filename = (\n",
    "            title.lower()\n",
    "                 .replace(\" \", \"_\")\n",
    "                 .replace(\"(\", \"\")\n",
    "                 .replace(\")\", \"\")\n",
    "                 .replace(\"-\", \"\")\n",
    "            + \".png\"\n",
    "        )\n",
    "        fig.savefig(filename, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    plot_pairs(normal_idx, \"Normal (0-8)\")\n",
    "    plot_pairs(anomaly_idx, \"Anomaly (9)\")\n",
    "\n",
    "def analyze_latent_dimensions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_mu = []\n",
    "    all_logvar = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in data_loader:\n",
    "            x = x.to(device)\n",
    "            mu, log_var = model.encode(x)\n",
    "            all_mu.append(mu.cpu())\n",
    "            all_logvar.append(log_var.cpu())\n",
    "\n",
    "    all_mu = torch.cat(all_mu)\n",
    "    all_logvar = torch.cat(all_logvar)\n",
    "\n",
    "    # Calculate KL divergence per dimension\n",
    "    kl_per_dim = -0.5 * (1 + all_logvar - all_mu**2 - all_logvar.exp())\n",
    "    kl_per_dim = kl_per_dim.mean(dim=0)\n",
    "\n",
    "    collapsed_dims = (kl_per_dim < 0.01).sum().item()\n",
    "\n",
    "    print(f\"Number of collapsed dimensions (KL < 0.01): {collapsed_dims}/{len(kl_per_dim)}\")\n",
    "\n",
    "def show_interpolation_artifacts(model, data_loader, device, num_steps=10):\n",
    "    \"\"\"Show VAE disadvantage: Unrealistic interpolations\"\"\"\n",
    "    model.eval()\n",
    "    x_batch, y_batch = next(iter(data_loader))\n",
    "\n",
    "    # Get two different digits\n",
    "    idx1 = (y_batch == 0).nonzero(as_tuple=True)[0][0]\n",
    "    idx2 = (y_batch == 8).nonzero(as_tuple=True)[0][0]\n",
    "\n",
    "    x1 = x_batch[idx1:idx1+1].to(device)\n",
    "    x2 = x_batch[idx2:idx2+1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu1, _ = model.encode(x1)\n",
    "        mu2, _ = model.encode(x2)\n",
    "\n",
    "        # Interpolate\n",
    "        alphas = torch.linspace(0, 1, num_steps)\n",
    "        interpolated = []\n",
    "        for alpha in alphas:\n",
    "            z = (1 - alpha) * mu1 + alpha * mu2\n",
    "            img = model.decode(z)\n",
    "            interpolated.append(img)\n",
    "\n",
    "        interpolated = torch.cat(interpolated)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(num_steps*1.5, 2))\n",
    "    for i in range(num_steps):\n",
    "        img = interpolated[i].cpu().permute(1, 2, 0)\n",
    "        img = (img + 1.0) / 2.0\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'{alphas[i]:.1f}', fontsize=8)\n",
    "\n",
    "    plt.suptitle('Interpolations\\n',\n",
    "                 fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vae_interpolation_artifacts.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "errors, labels = compute_reconstruction_errors(model, test_loader, device)\n",
    "plot_anomaly_histogram(errors, labels)\n",
    "\n",
    "show_reconstructions(model, test_loader, device)\n",
    "\n",
    "analyze_latent_dimensions(model, test_loader, device)\n",
    "\n",
    "show_interpolation_artifacts(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
